# -*- coding: utf-8 -*-
"""BackupIntegrity-Verifier — Backup Integrity Checker for Cloud Storage

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y1sEyvWd3UiDWmKNrdVAn4E3hqiXSSDK
"""

# ============================================================
# BackupIntegrity-Verifier — Backup Integrity Checker for Cloud Storage (Colab ready)
# ============================================================

import os
import json
import hashlib
import time
from datetime import datetime, timezone

# Configuration
BACKUP_DIR = "data/backups"
METADATA_FILE = os.path.join(BACKUP_DIR, "backup_metadata.json")
REPORT_FILE = "data/backup_integrity_report.json"

CHECKSUM_ALGORITHM = "sha256"
STALE_THRESHOLD_DAYS = 30  # mark backup as stale if older than this

# Ensure directories
os.makedirs(BACKUP_DIR, exist_ok=True)
os.makedirs(os.path.dirname(REPORT_FILE), exist_ok=True)

# Generate dummy backup files and metadata if not exist
if not os.path.exists(METADATA_FILE):
    metadata = []
    now = datetime.now(timezone.utc)
    for i in range(1, 11):
        filename = f"backup_file_{i:03}.bin"
        filepath = os.path.join(BACKUP_DIR, filename)
        # Write dummy file data
        with open(filepath, "wb") as f:
            f.write(os.urandom(1024 * i))  # file size grows with i
        # Compute checksum
        hasher = hashlib.new(CHECKSUM_ALGORITHM)
        with open(filepath, "rb") as f:
            buf = f.read()
            hasher.update(buf)
        checksum = hasher.hexdigest()
        created = now.timestamp() - (i * 86400)  # simulate files created i days ago
        metadata.append({
            "filename": filename,
            "checksum": checksum,
            "created_at": datetime.fromtimestamp(created, timezone.utc).isoformat()
        })
    with open(METADATA_FILE, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2)

def load_metadata(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def compute_checksum(path, algorithm="sha256"):
    hasher = hashlib.new(algorithm)
    with open(path, "rb") as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()

def analyze_backups(metadata, backup_dir, stale_days):
    report = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "total_backups": len(metadata),
        "integrity_ok": 0,
        "integrity_failures": 0,
        "stale_backups": 0,
        "details": []
    }
    now = datetime.now(timezone.utc)
    for entry in metadata:
        fname = entry["filename"]
        expected_checksum = entry["checksum"]
        filepath = os.path.join(backup_dir, fname)
        issues = []
        if not os.path.exists(filepath):
            issues.append("file_missing")
        else:
            actual_checksum = compute_checksum(filepath)
            if actual_checksum != expected_checksum:
                issues.append("checksum_mismatch")
        # stale check
        created_at = datetime.fromisoformat(entry["created_at"])
        age_days = (now - created_at).days
        if age_days > stale_days:
            issues.append("stale_backup")
            report["stale_backups"] += 1
        # aggregate
        if not issues:
            report["integrity_ok"] += 1
        else:
            report["integrity_failures"] += 1
            report["details"].append({
                "filename": fname,
                "issues": issues,
                "age_days": age_days
            })
    return report

def run_audit(metadata_path, backup_dir, report_path, stale_days):
    metadata = load_metadata(metadata_path)
    report = analyze_backups(metadata, backup_dir, stale_days)
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2)
    return report

if __name__ == "__main__":
    result = run_audit(METADATA_FILE, BACKUP_DIR, REPORT_FILE, STALE_THRESHOLD_DAYS)
    print("✅ Backup integrity audit complete:", result)